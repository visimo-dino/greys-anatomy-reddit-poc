{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime as dt\n",
    "from searchtweets import ResultStream, gen_rule_payload, collect_results, load_credentials\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Twitter credentials from YAML file.\n",
    "\n",
    "# Credentials for fetching actual tweets.\n",
    "search_args = load_credentials(\n",
    "    'twitter_keys.yaml',\n",
    "    yaml_key='search_tweets_premium',\n",
    "    env_overwrite=False,\n",
    ")\n",
    "\n",
    "# Credentials for fetching counts of tweets.\n",
    "count_args = load_credentials(\n",
    "    'twitter_keys.yaml',\n",
    "    yaml_key='tweet_counts_premium',\n",
    "    env_overwrite=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keywords for each category (Grey's Anatomy + abortion).\n",
    "greys_words = [\n",
    "    'greysanatomy',\n",
    "    '#greysanatomy',\n",
    "    '\"greys anatomy\"',\n",
    "    '\"grey\\'s anatomy\"',\n",
    "    '@GreysABC',\n",
    "    '#greys',\n",
    "    '\"meredith grey\"',\n",
    "    '\"derek shepherd\"',\n",
    "]\n",
    "\n",
    "abortion_words = [\n",
    "    'abortion',\n",
    "    '#abortion',\n",
    "    'prochoice',\n",
    "    'pro-choice',\n",
    "    '#prochoice',\n",
    "    '#pro-choice',\n",
    "    'prolife',\n",
    "    'pro-life',\n",
    "    '#prolife',\n",
    "    '#pro-life',\n",
    "    '\"roe v wade\"',\n",
    "    '\"roe vs wade\"',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define three queries: one for Grey's Anatomy, one for abortion, and for tweets that contain keywords from both lists.\n",
    "greys_q =  f\"({' OR '.join(greys_words)})\"\n",
    "abortion_q =  f\"({' OR '.join(abortion_words)})\"\n",
    "both_q = f\"{greys_q} {abortion_q}\"\n",
    "\n",
    "# Add english language restriction to all three queries.\n",
    "greys_q += ' lang:en'\n",
    "abortion_q += ' lang:en'\n",
    "both_q += ' lang:en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'rule' which is the JSON that gets sent to Twitter's endpoints-- one for each query.\n",
    "greys_rule = gen_rule_payload(\n",
    "    greys_q,\n",
    "    from_date='2019-10-24',\n",
    "    to_date='2019-11-21',\n",
    "#     count_bucket='day',  <- uncomment this parameter if you're getting counts rather than tweets\n",
    ")\n",
    "\n",
    "abortion_rule = gen_rule_payload(\n",
    "    abortion_q,\n",
    "    from_date='2019-10-24',\n",
    "    to_date='2019-11-21',\n",
    "#     count_bucket='day',  <- uncomment this parameter if you're getting counts rather than tweets\n",
    ")\n",
    "\n",
    "both_rule = gen_rule_payload(\n",
    "    both_q,\n",
    "    from_date='2019-10-24',\n",
    "    to_date='2019-11-21',\n",
    "#     count_bucket='day',  <- uncomment this parameter if you're getting counts rather than tweets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convenience, copy of above cell but with the count_bucket parameter uncommented, for fetching counts.\n",
    "greys_rule = gen_rule_payload(\n",
    "    greys_q,\n",
    "    from_date='2019-10-24',\n",
    "    to_date='2019-11-21',\n",
    "    count_bucket='day',\n",
    ")\n",
    "\n",
    "abortion_rule = gen_rule_payload(\n",
    "    abortion_q,\n",
    "    from_date='2019-10-24',\n",
    "    to_date='2019-11-21',\n",
    "    count_bucket='day',\n",
    ")\n",
    "\n",
    "both_rule = gen_rule_payload(\n",
    "    both_q,\n",
    "    from_date='2019-10-24',\n",
    "    to_date='2019-11-21',\n",
    "    count_bucket='day',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total counts for each of the three queries.\n",
    "greys_counts = collect_results(\n",
    "    greys_rule,\n",
    "    max_results=1000,\n",
    "    result_stream_args=count_args,\n",
    ")\n",
    "\n",
    "abortion_counts = collect_results(\n",
    "    abortion_rule,\n",
    "    max_results=1000,\n",
    "    result_stream_args=count_args,\n",
    ")\n",
    "\n",
    "both_counts = collect_results(\n",
    "    both_rule,\n",
    "    max_results=1000,\n",
    "    result_stream_args=count_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert counts data into DataFrames.\n",
    "greys_df = pd.DataFrame(greys_counts)\n",
    "abortion_df = pd.DataFrame(abortion_counts)\n",
    "both_df = pd.DataFrame(both_counts)\n",
    "\n",
    "# Format timePeriod column as a datetime.\n",
    "greys_df['timePeriod'] = pd.to_datetime(greys_df.timePeriod)\n",
    "abortion_df['timePeriod'] = pd.to_datetime(abortion_df.timePeriod)\n",
    "both_df['timePeriod'] = pd.to_datetime(both_df.timePeriod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all three DataFrames to CSV.\n",
    "greys_df.to_csv('./greys.csv', index=None)\n",
    "abortion_df.to_csv('./abortion.csv', index=None)\n",
    "both_df.to_csv('./both.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, gather 500 actual tweets using the Grey's Anatomy terms to validate that the data is good.\n",
    "greys_tweets = collect_results(\n",
    "    greys_rule,\n",
    "    max_results=500,\n",
    "    result_stream_args=search_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting the useful information out of the tweet JSON and adding to a dict.\n",
    "def parse_tweet(tweet):\n",
    "    d = {}\n",
    "    d['created_at'] = tweet['created_at']\n",
    "    d['tweet_id'] = tweet['id_str']\n",
    "    try:\n",
    "        d['text'] = tweet['extended_tweet']['full_text']\n",
    "    except KeyError:\n",
    "        d['text'] = tweet['text']\n",
    "    d['screen_name'] = tweet['user']['screen_name']\n",
    "    d['location'] = tweet['user']['location']\n",
    "    d['tweet_url'] = f'https://twitter.com/{d[\"screen_name\"]}/status/{d[\"tweet_id\"]}'\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse all of the Grey's Anatomy tweets.\n",
    "parsed_tweets = [parse_tweet(tweet) for tweet in greys_tweets]\n",
    "\n",
    "# Convert the parsed tweets into a DataFrame.\n",
    "greys_tweets_df = pd.DataFrame(parsed_tweets)\n",
    "\n",
    "# Convert created_at column to datetime, then remove timezone info so Excel doesn't have issues.\n",
    "greys_tweets_df['created_at'] = pd.to_datetime(greys_tweets_df.created_at).map(lambda x: x.replace(tzinfo=None))\n",
    "\n",
    "# Convert tweet_id column to string so Excel doesn't have formatting issues.\n",
    "greys_tweets_df['tweet_id'] = greys_tweets_df.tweet_id.astype(str)\n",
    "\n",
    "# Write Grey's Anatomy tweets DataFrame to Excel file.\n",
    "greys_tweets_df.to_excel('greys_tweets.xlsx', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
